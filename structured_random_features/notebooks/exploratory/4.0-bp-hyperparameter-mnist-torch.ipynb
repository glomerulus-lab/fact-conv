{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd\n",
    "from os import path\n",
    "from copy import deepcopy\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.load_dataset import load_mnist, load_kmnist\n",
    "from src.models.networks import V1_mnist_RFNet, classical_RFNet\n",
    "from src.models.utils import train, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = path.abspath(path.join(getcwd(), '../../'))\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1_RFNet = V1_mnist_RFNet(100, 5.0, 2.0, center=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V1_RFNet_objective(trial):\n",
    "    \n",
    "    # load the data\n",
    "    train_loader, val_loader, _ = load_mnist(train_batch_size=batch_size, train_percentage=0.2)\n",
    "    \n",
    "    # load the model\n",
    "    s = trial.suggest_uniform(\"size\", 0.01,  10)\n",
    "    f = trial.suggest_uniform(\"frequency\", 0.01, 10)\n",
    "    model = V1_mnist_RFNet(100, s, f, center=None).to(device)\n",
    "    \n",
    "    # generate optimizers, learning rate, and the loss function\n",
    "    lr = 0.0031485838088746586\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) \n",
    "    loss_fn = F.cross_entropy\n",
    "\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True) \n",
    "#     optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "#     optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "#     loss_fn = F.cross_entropy_loss\n",
    "\n",
    "    \n",
    "    # train and validate\n",
    "    for epoch in range(epochs + 1):\n",
    "        _ = train(log_interval, device, model, train_loader, optimizer, epoch, loss_fn, verbose=False)\n",
    "        val_accuracy = test(model, device, val_loader, loss_fn, verbose=False)\n",
    "        \n",
    "        trial.report(val_accuracy, epoch)\n",
    "    \n",
    "        prune if unpromising trial\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hyperparameter study\n",
    "v1_study = optuna.create_study(sampler=optuna.samplers.RandomSampler(), direction='maximize')\n",
    "v1_study.optimize(V1_RFNet_objective, n_trials=50)\n",
    "\n",
    "# save the parameter study\n",
    "joblib.dump(v1_study, data_dir + '/models/results/mnist_clf/mnist_param_study.pkl')\n",
    "\n",
    "pruned_trials = v1_study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = v1_study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(v1_study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = v1_study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the parameter study\n",
    "with open(data_dir + '/models/results/mnist_clf/mnist_param_study.pkl', 'rb') as file:\n",
    "    v1_study = joblib.load(file)\n",
    "\n",
    "fig = optuna.visualization.plot_contour(v1_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "network_500 = V1_mnist_RFNet(500, 5.34, 1.965, None).to(device)\n",
    "print('Done')\n",
    "network_600 = V1_mnist_RFNet(600, 5.34, 1.965, None).to(device)\n",
    "print('Done')\n",
    "network_1000 = V1_mnist_RFNet(1000, 5.34, 1.965, None).to(device)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_epoch: 0 [0/600 (0%)]\tLoss: 66.979279\n",
      "Train_epoch: 1 [0/600 (0%)]\tLoss: 20.509905\n",
      "Train_epoch: 2 [0/600 (0%)]\tLoss: 11.392003\n",
      "Train_epoch: 3 [0/600 (0%)]\tLoss: 10.950925\n",
      "Train_epoch: 4 [0/600 (0%)]\tLoss: 3.472730\n",
      "Train_epoch: 5 [0/600 (0%)]\tLoss: 2.842002\n",
      "Train_epoch: 6 [0/600 (0%)]\tLoss: 0.084602\n",
      "Train_epoch: 7 [0/600 (0%)]\tLoss: 0.000361\n",
      "Train_epoch: 8 [0/600 (0%)]\tLoss: 0.234045\n",
      "Train_epoch: 9 [0/600 (0%)]\tLoss: 0.405027\n",
      "\n",
      "Test set: Average loss: 27.567228. Accuracy: 6570/10000 (65.70%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model using the most optimal hyperparams\n",
    "hidden_size = 1000\n",
    "s, f, c = 5.34, 1.965, None\n",
    "lr = 0.0031485838088746586\n",
    "num_epochs = 10\n",
    "log_interval = 100\n",
    "\n",
    "# define the model, optimize, loss\n",
    "model = deepcopy(network_1000)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# load data\n",
    "train_batch_size, train_percentage = 64, 0.01\n",
    "train_loader, val_loader, test_loader = load_kmnist(train_batch_size, train_percentage)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    _ = train(log_interval, device, model, train_loader, optimizer, epoch, loss_fn, verbose=True)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        test_accuracy = test(model, device, test_loader, loss_fn, verbose=True)\n",
    "        \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.030000000000001"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classical network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (1, 28, 28)\n",
    "hidden_size = 500\n",
    "def classical_RFNet_objective(trial):\n",
    "    \n",
    "    # load the data\n",
    "    train_loader, val_loader, _ = load_mnist(train_batch_size=batch_size, train_percentage=0.2)\n",
    "    \n",
    "    # load the model\n",
    "    model = classical_RFNet(inp_size, hidden_size).to(device)\n",
    "    \n",
    "    # generate optimizers and the learning rate\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True) \n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    # train and validate\n",
    "    for epoch in range(epochs + 1):\n",
    "        _ = train(log_interval, device, model, train_loader, optimizer, epoch, verbose=False)\n",
    "        val_accuracy = test(model, device, val_loader, verbose=False)\n",
    "        \n",
    "        trial.report(val_accuracy, epoch)\n",
    "    \n",
    "        # prune if unpromising trial\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hyperparameter study\n",
    "classical_study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction='maximize')\n",
    "classical_study.optimize(classical_RFNet_objective, n_trials=50)\n",
    "\n",
    "pruned_trials = classical_study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = classical_study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(classical_study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = classical_study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_epoch: 0 [0/600 (0%)]\tLoss: 2.326934\n",
      "Train_epoch: 1 [0/600 (0%)]\tLoss: 0.706675\n",
      "Train_epoch: 2 [0/600 (0%)]\tLoss: 0.367240\n",
      "Train_epoch: 3 [0/600 (0%)]\tLoss: 0.311689\n",
      "Train_epoch: 4 [0/600 (0%)]\tLoss: 0.112709\n",
      "Train_epoch: 5 [0/600 (0%)]\tLoss: 0.037038\n",
      "Train_epoch: 6 [0/600 (0%)]\tLoss: 0.039064\n",
      "Train_epoch: 7 [0/600 (0%)]\tLoss: 0.042801\n",
      "Train_epoch: 8 [0/600 (0%)]\tLoss: 0.051458\n",
      "Train_epoch: 9 [0/600 (0%)]\tLoss: 0.026095\n",
      "\n",
      "Test set: Average loss: 1.818231. Accuracy: 6127/10000 (61.27%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model using the most optimal hyperparams\n",
    "inp_size = (1, 28, 28)\n",
    "hidden_size = 1000\n",
    "lr = 0.01922083004518646\n",
    "num_epochs = 10\n",
    "log_interval = 100\n",
    "\n",
    "# define the model\n",
    "model = classical_RFNet(inp_size, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# load data\n",
    "train_batch_size, train_percentage = 64, 0.01\n",
    "train_loader, val_loader, test_loader = load_kmnist(train_batch_size, train_percentage)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    _ = train(log_interval, device, model, train_loader, optimizer, epoch, loss_fn, verbose=True)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        test_accuracy = test(model, device, test_loader, loss_fn, verbose=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.86"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00019220830045186461]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
