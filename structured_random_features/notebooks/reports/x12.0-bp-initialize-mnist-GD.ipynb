{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7316a5a-e5a5-40a1-93e4-d3855a8b87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whitev4/anaconda3/envs/random_features/lib/python3.8/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from os import getcwd, makedirs\n",
    "from os.path import abspath, join, exists\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.networks import V1_mnist_RFNet, classical_RFNet\n",
    "from src.data.load_dataset import load_mnist\n",
    "import kymatio.datasets as scattering_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a3499e3-6a86-4761-b6f1-f3a0c2c4265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = abspath(join(getcwd(), '../../'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29163546-b9c8-4c8c-acc6-33432f97884e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CIFAR10' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m train_set \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39mscattering_datasets\u001b[38;5;241m.\u001b[39mget_dataset_dir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR\u001b[39m\u001b[38;5;124m'\u001b[39m), train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m test_set \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39mscattering_datasets\u001b[38;5;241m.\u001b[39mget_dataset_dir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR\u001b[39m\u001b[38;5;124m'\u001b[39m), train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device), train_set\u001b[38;5;241m.\u001b[39mtargets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m test, y_test \u001b[38;5;241m=\u001b[39m test_set\u001b[38;5;241m.\u001b[39mto(device), test_set\u001b[38;5;241m.\u001b[39mtargets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# normalize and reshape\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CIFAR10' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# load dataset and move to GPU\n",
    "mnist_dir = data_dir + '/data/processed/'\n",
    "train_set = datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=True, download=False)\n",
    "test_set = datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=False, download=False)\n",
    "train, y_train = train_set.to(device), train_set.targets.to(device)\n",
    "test, y_test = test_set.to(device), test_set.targets.to(device)\n",
    "\n",
    "# normalize and reshape\n",
    "X_train = (train - train.mean()) / train.std()\n",
    "X_test = (test - train.mean()) / train.std()\n",
    "X_train = X_train.view(-1, 1, 28, 28)\n",
    "X_test = X_test.view(-1, 1, 28, 28)\n",
    "\n",
    "# training params\n",
    "scale = 2/784 # since we do a cholesky before generating weights\n",
    "num_epochs = 10000\n",
    "num_trials = 5\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# params to iterate over\n",
    "hidden_size_list = [50, 100, 400, 1000, 2000, 5000]\n",
    "lr_list = [1E-1, 1E-2, 1E-3]\n",
    "\n",
    "# V1 params\n",
    "compatible = {'s': 5, 'f':2}\n",
    "incompatible = {'s': 0.5, 'f':0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6525f-77c4-4e0a-88bd-6aa0fee9768b",
   "metadata": {},
   "source": [
    "### V1 RFNet with optimal parameters $s=5$ and $f=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ffdb4-36a4-4672-b9ce-675ab6a689e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, f = compatible['s'], compatible['f']\n",
    "v1_train_loss = {h: {lr: {'mean': [], 'std': []} for lr in lr_list} for h in hidden_size_list}\n",
    "v1_test_accuracy = {h: {lr: {'mean': [], 'std': []} for lr in lr_list} for h in hidden_size_list}\n",
    "\n",
    "for h in hidden_size_list:\n",
    "    for lr in lr_list:\n",
    "        train_loss = np.zeros((num_trials, num_epochs))\n",
    "        test_accuracy = np.zeros((num_trials, num_epochs))\n",
    "        for trial in range(num_trials):\n",
    "            # declare model and optimizer\n",
    "            model = V1_mnist_RFNet(h, s, f, scale=scale, center=None).to(device)\n",
    "            model.v1_layer.weight.requires_grad = True\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_train)\n",
    "                loss = loss_fn(output, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss[trial, epoch] = loss.item()\n",
    "\n",
    "                # accuracy\n",
    "                pred = torch.argmax(model(X_test), axis=1)\n",
    "                test_accuracy[trial, epoch] = torch.sum(pred == y_test) / len(X_test)\n",
    "\n",
    "                if epoch % 1000 == 0:\n",
    "                    print('Trial: {}\\tTrain_epoch: {}\\tLoss: {:.6f}'.format(trial, epoch, loss.item()))\n",
    "    \n",
    "        # train error\n",
    "        v1_train_loss[h][lr]['mean'] = np.mean(train_loss, axis=0)\n",
    "        v1_train_loss[h][lr]['std'] = np.std(train_loss, axis=0) / np.sqrt(num_trials)\n",
    "        # test error\n",
    "        v1_test_accuracy[h][lr]['mean'] = np.mean(test_accuracy, axis=0)\n",
    "        v1_test_accuracy[h][lr]['std'] = np.std(test_accuracy, axis=0) / np.sqrt(num_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0548a4-f3fc-4a63-820c-d36fcc4a900f",
   "metadata": {},
   "source": [
    "### classical weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c33a68-eb70-479c-a4e6-a65d61da2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (1, 28, 28)\n",
    "classical_train_loss = {h: {lr: {'mean': [], 'std': []} for lr in lr_list} for h in hidden_size_list}\n",
    "classical_test_accuracy = {h: {lr: {'mean': [], 'std': []} for lr in lr_list} for h in hidden_size_list}\n",
    "\n",
    "for h in hidden_size_list:\n",
    "    for lr in lr_list:\n",
    "        train_loss = np.zeros((num_trials, num_epochs))\n",
    "        test_accuracy = np.zeros((num_trials, num_epochs))\n",
    "        for trial in range(num_trials):\n",
    "            # define the model and optimizer\n",
    "            model = classical_RFNet(inp_size, h, scale=scale).to(device)\n",
    "            model.RF_layer.weight.requires_grad = True\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_train)\n",
    "                loss = loss_fn(output, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss[trial, epoch] = loss.item()\n",
    "\n",
    "                # accuracy\n",
    "                pred = torch.argmax(model(X_test), axis=1)\n",
    "                test_accuracy[trial, epoch] = torch.sum(pred == y_test) / len(X_test)\n",
    "\n",
    "                if epoch % 1000 == 0:\n",
    "                    print('Trial: {}\\tTrain_epoch: {}\\tLoss: {:.6f}'.format(trial, epoch, loss.item()))\n",
    "                    \n",
    "        # train error\n",
    "        classical_train_loss[h][lr]['mean'] = np.mean(train_loss, axis=0)\n",
    "        classical_train_loss[h][lr]['std'] = np.std(train_loss, axis=0) / np.sqrt(num_trials)\n",
    "        \n",
    "        # test error\n",
    "        classical_test_accuracy[h][lr]['mean'] = np.mean(test_accuracy, axis=0)\n",
    "        classical_test_accuracy[h][lr]['std'] = np.std(test_accuracy, axis=0) / np.sqrt(num_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681b6ee-b91f-4cfc-9ba6-8f680ffd86c6",
   "metadata": {},
   "source": [
    "### Incompatible RFNet with parameters $s=0.5$ and $f=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336fe4da-4a49-4149-a3c0-11482cb46bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, f = incompatible['s'], incompatible['f']\n",
    "incompatible_train_loss = {h: {lr: {'mean': [], 'std': []} for lr in lr_list} for h in hidden_size_list}\n",
    "incompatible_test_accuracy = {h: {lr: {'mean': [], 'std': []} for lr in lr_list} for h in hidden_size_list}\n",
    "\n",
    "for h in hidden_size_list:\n",
    "    for lr in lr_list:\n",
    "        train_loss = np.zeros((num_trials, num_epochs))\n",
    "        test_accuracy = np.zeros((num_trials, num_epochs))\n",
    "        for trial in range(num_trials):\n",
    "            # declare model and optimizer\n",
    "            model = V1_mnist_RFNet(h, s, f, scale=scale, center=None).to(device)\n",
    "            model.v1_layer.weight.requires_grad = True\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_train)\n",
    "                loss = loss_fn(output, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss[trial, epoch] = loss.item()\n",
    "\n",
    "                # accuracy\n",
    "                pred = torch.argmax(model(X_test), axis=1)\n",
    "                test_accuracy[trial, epoch] = torch.sum(pred == y_test) / len(X_test)\n",
    "\n",
    "                if epoch % 1000 == 0:\n",
    "                    print('Trial: {}\\tTrain_epoch: {}\\tLoss: {:.6f}'.format(trial, epoch, loss.item()))\n",
    "                    \n",
    "        # train error\n",
    "        incompatible_train_loss[h][lr]['mean'] = np.mean(train_loss, axis=0)\n",
    "        incompatible_train_loss[h][lr]['std'] = np.std(train_loss, axis=0) / np.sqrt(num_trials)\n",
    "        \n",
    "        # test error\n",
    "        incompatible_test_accuracy[h][lr]['mean'] = np.mean(test_accuracy, axis=0)\n",
    "        incompatible_test_accuracy[h][lr]['std'] = np.std(test_accuracy, axis=0) / np.sqrt(num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa87d35-9678-40a8-9d5d-e903acb9d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "s, f = compatible['s'], compatible['f']\n",
    "results = {}\n",
    "results['V1'] = {'train_loss': v1_train_loss, 'test_accuracy': v1_test_accuracy}\n",
    "results['classical'] = {'train_loss': classical_train_loss, 'test_accuracy': classical_test_accuracy}\n",
    "results['incompatible'] = {'train_loss': incompatible_train_loss, 'test_accuracy': incompatible_test_accuracy}\n",
    "\n",
    "save_dir = data_dir + '/models/results/initialize_mnist'\n",
    "if not exists(save_dir):\n",
    "    makedirs(save_dir)\n",
    "with open(save_dir + '/clf_s=%0.2f_f=%0.2f_GD_torch.pickle' % (s, f), 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a737893-d89d-42d0-8cce-5046931388e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "compatible = {'s': 5, 'f':2}\n",
    "s, f = compatible['s'], compatible['f']\n",
    "with open(save_dir + '/clf_s=%0.2f_f=%0.2f_GD_torch.pickle' % (s, f), 'rb') as handle:\n",
    "    test = pickle.load(handle)\n",
    "    \n",
    "h, lr = 1000, 0.1\n",
    "fig = plt.figure()\n",
    "# plt.ylim(0, 0.2)\n",
    "plt.semilogy(1 - test['V1']['test_accuracy'][h][lr]['mean'])\n",
    "plt.semilogy(1 - test['classical']['test_accuracy'][h][lr]['mean'])\n",
    "plt.semilogy(1 - test['incompatible']['test_accuracy'][h][lr]['mean'])\n",
    "plt.ylabel('test error')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "fig = plt.figure()\n",
    "# plt.ylim(0, 0.2)\n",
    "plt.semilogy(test['V1']['train_loss'][h][lr]['mean'])\n",
    "plt.semilogy(test['classical']['train_loss'][h][lr]['mean'])\n",
    "plt.semilogy(test['incompatible']['train_loss'][h][lr]['mean'])\n",
    "plt.ylabel('train loss')\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21c6b1-26ea-4b50-a5c7-0c626a042132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
